{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQesaUP/1lJ73RksnQd5Lj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"cG_YpwrI4r0C"}},{"cell_type":"code","source":["corpus = [\n","    \"I can't wait for the new season of my favorite show!\",\n","    \"The pCOVID-19 andemic has affected millions of people worldwide.\",\n","    \"U.S. stocks fell on Friday after news of rising inflation.\",\n","    \"<html><body>Welcome to the website!</body></html>\",\n","    \"Python is a great programming language!!! ??\"\n","]"],"metadata":{"id":"hXB8yBaF4zeS","executionInfo":{"status":"ok","timestamp":1757864773724,"user_tz":-330,"elapsed":6,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["1. Text Cleaning\n","We'll convert the text to lowercase, remove punctuation, numbers, special characters and HTML tags.\n","\n","Defines a clean_text() function to clean and normalize raw text data for NLP tasks.\n","Applies clean_text() to every document in the corpus list using a list comprehension.\n","Stores the cleaned version of all documents in a new list called cleaned_corpus.\n","Prints cleaned_corpus which is ready for tokenization."],"metadata":{"id":"BoB7OSFp40P3"}},{"cell_type":"code","source":["import re #regular expressions (for pattern matching and cleaning text)\n","import string #gives access to punctuation characters\n","from bs4 import BeautifulSoup  #removes HTML tags from text\n","\n","def clean_text(text):\n","    text = text.lower()  # Lowercase\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n","    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n","    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n","    return text\n","\n","cleaned_corpus = [clean_text(doc) for doc in corpus]\n","print(cleaned_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAwYVNWu43Cj","executionInfo":{"status":"ok","timestamp":1757864773747,"user_tz":-330,"elapsed":18,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}},"outputId":"682e627d-15b3-49a6-f60c-383a567065ef"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['i cant wait for the new season of my favorite show', 'the covid pandemic has affected millions of people worldwide', 'us stocks fell on friday after news of rising inflation', 'htmlbodywelcome to the websitebodyhtml', 'python is a great programming language ']\n"]}]},{"cell_type":"markdown","source":["2. Tokenization\n","Splitting the cleaned text into tokens (words).\n","Imports word_tokenize to split text into individual words.\n","Downloads the necessary NLTK tokenizer model ('punkt_tab' ).\n","Tokenizes each cleaned document in cleaned_corpus.\n","Stores the list of tokens for each document in tokenized_corpus.\n","Prints the final tokenized output(a list of word lists)."],"metadata":{"id":"LFmbx1sh49Fh"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt_tab')\n","\n","tokenized_corpus = [word_tokenize(doc) for doc in cleaned_corpus]\n","print(tokenized_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SsC094B65AS7","executionInfo":{"status":"ok","timestamp":1757864773750,"user_tz":-330,"elapsed":13,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}},"outputId":"460202fd-9fc1-4724-d873-c9626a6d9d9c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[['i', 'cant', 'wait', 'for', 'the', 'new', 'season', 'of', 'my', 'favorite', 'show'], ['the', 'covid', 'pandemic', 'has', 'affected', 'millions', 'of', 'people', 'worldwide'], ['us', 'stocks', 'fell', 'on', 'friday', 'after', 'news', 'of', 'rising', 'inflation'], ['htmlbodywelcome', 'to', 'the', 'websitebodyhtml'], ['python', 'is', 'a', 'great', 'programming', 'language']]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["3. Stop Words Removal\n","Removing common stop words from the tokens.\n","\n","Imports the list of English stopwords from nltk.corpus.stopwords.\n","Downloads the stopwords corpus using nltk.download('stopwords').\n","Stores all English stopwords (like \"the\", \"is\", \"and\", etc.) in a set called stop_words for fast lookup.\n","Iterates over each document in tokenized_corpus and removes all stopwords.\n","Saves the cleaned, non-stopword tokens into filtered_corpus.\n","Prints the a list of documents."],"metadata":{"id":"eHpfo_fm5DIi"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","filtered_corpus = [[word for word in doc if word not in stop_words] for doc in tokenized_corpus]\n","print(filtered_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHEUph7T5F09","executionInfo":{"status":"ok","timestamp":1757864773762,"user_tz":-330,"elapsed":11,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}},"outputId":"86825577-393a-45a5-ba67-394c3229dd24"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[['cant', 'wait', 'new', 'season', 'favorite', 'show'], ['covid', 'pandemic', 'affected', 'millions', 'people', 'worldwide'], ['us', 'stocks', 'fell', 'friday', 'news', 'rising', 'inflation'], ['htmlbodywelcome', 'websitebodyhtml'], ['python', 'great', 'programming', 'language']]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["4. Stemming and Lemmatization\n","Reducing words to their base form using stemming and lemmatization.\n","\n","Imports PorterStemmer and WordNetLemmatizer from NLTK.\n","Downloads the wordnet resource required for lemmatization.\n","Initializes the stemmer and lemmatizer.\n","Applies stemming to each word in filtered_corpus and stores the result in stemmed_corpus.\n","Applies lemmatization to each word in filtered_corpus and stores the result in lemmatized_corpus.\n","Prints both the stemmed and lemmatized versions of the corpus."],"metadata":{"id":"9v_S2BYm5ILM"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer, WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","stemmed_corpus = [[stemmer.stem(word) for word in doc] for doc in filtered_corpus]\n","lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in filtered_corpus]\n","print(stemmed_corpus)\n","print(lemmatized_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIhaTBvx5Kz3","executionInfo":{"status":"ok","timestamp":1757864773772,"user_tz":-330,"elapsed":8,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}},"outputId":"688f769d-2f25-463a-b1de-3621b3ef7c2c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[['cant', 'wait', 'new', 'season', 'favorit', 'show'], ['covid', 'pandem', 'affect', 'million', 'peopl', 'worldwid'], ['us', 'stock', 'fell', 'friday', 'news', 'rise', 'inflat'], ['htmlbodywelcom', 'websitebodyhtml'], ['python', 'great', 'program', 'languag']]\n","[['cant', 'wait', 'new', 'season', 'favorite', 'show'], ['covid', 'pandemic', 'affected', 'million', 'people', 'worldwide'], ['u', 'stock', 'fell', 'friday', 'news', 'rising', 'inflation'], ['htmlbodywelcome', 'websitebodyhtml'], ['python', 'great', 'programming', 'language']]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]}]}