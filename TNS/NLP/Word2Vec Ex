{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNhYiH/2+ubmI2W0gmueHN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0GfKXTmjvIS","executionInfo":{"status":"ok","timestamp":1757875906989,"user_tz":-330,"elapsed":382,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}},"outputId":"54403009-441b-49ac-b63a-e62da9be4a6c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Tokenized: ['word', 'embeddings', 'are', 'dense', 'vector', 'representations', 'of', 'words', '.']\n","Vector for 'word':\n","[-0.01427803  0.00248206 -0.01435343 -0.00448924  0.00743861  0.01166625\n","  0.00239637  0.00420546 -0.00822078  0.01445067 -0.01261408  0.00929443\n"," -0.01643995  0.00407294 -0.0099541  -0.00849538 -0.00621797  0.01131042\n","  0.0115968  -0.0099493   0.00154666 -0.01699156  0.01561961  0.01851458\n"," -0.00548466  0.00160045  0.0014933   0.01095577 -0.01721216  0.00116891\n","  0.01373884  0.00446319  0.00224935 -0.01864431  0.01696473 -0.01252825\n"," -0.00598475  0.00698757 -0.00154526  0.00282258  0.00356398 -0.0136578\n"," -0.01944962  0.01808117  0.01239611 -0.01382586  0.00680696  0.00041213\n","  0.00950749 -0.01423989]\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","\n","# Download tokenizer models (fix for LookupError)\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","sample = \"Word embeddings are dense vector representations of words.\"\n","\n","# Tokenization\n","tokenized_corpus = word_tokenize(sample.lower())\n","print(\"Tokenized:\", tokenized_corpus)\n","\n","# Train a simple Skip-Gram Word2Vec model\n","skipgram_model = Word2Vec(\n","    sentences=[tokenized_corpus],\n","    vector_size=50,\n","    window=3,\n","    sg=1,  # sg=1 => Skip-Gram, sg=0 => CBOW\n","    min_count=1,\n","    workers=2\n",")\n","\n","# View embedding for the word 'word'\n","print(\"Vector for 'word':\")\n","print(skipgram_model.wv['word'])\n"]}]}