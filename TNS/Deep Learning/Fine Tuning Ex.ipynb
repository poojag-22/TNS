{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPrLMeg5rldm3jKEQ2CZO49"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2c45d1b9de84423391bf33f3f94df6dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c6495f23df24418bb6e5fbbe2f53395","IPY_MODEL_669785fe12db492c98e685a838243a03","IPY_MODEL_4dbdf3a8e8114ab7882ad8df5fb96f3b"],"layout":"IPY_MODEL_46e806db2b614160b27c1bed9dbc5e8b"}},"1c6495f23df24418bb6e5fbbe2f53395":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8bf71780b5b49818030690cfd590a67","placeholder":"​","style":"IPY_MODEL_9577a2fd14a543c5b4a7557ca713be90","value":"Map: 100%"}},"669785fe12db492c98e685a838243a03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b14dbe9a87074d0587c37191cea2f4d1","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b72f18b914348eb84c74e6260ddb927","value":200}},"4dbdf3a8e8114ab7882ad8df5fb96f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66e9fece8d29418c93fd9fa46a04e29d","placeholder":"​","style":"IPY_MODEL_2be40fe19de14eaf8e204679d21737da","value":" 200/200 [00:00&lt;00:00, 1295.74 examples/s]"}},"46e806db2b614160b27c1bed9dbc5e8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8bf71780b5b49818030690cfd590a67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9577a2fd14a543c5b4a7557ca713be90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b14dbe9a87074d0587c37191cea2f4d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b72f18b914348eb84c74e6260ddb927":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66e9fece8d29418c93fd9fa46a04e29d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2be40fe19de14eaf8e204679d21737da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2c45d1b9de84423391bf33f3f94df6dc","1c6495f23df24418bb6e5fbbe2f53395","669785fe12db492c98e685a838243a03","4dbdf3a8e8114ab7882ad8df5fb96f3b","46e806db2b614160b27c1bed9dbc5e8b","a8bf71780b5b49818030690cfd590a67","9577a2fd14a543c5b4a7557ca713be90","b14dbe9a87074d0587c37191cea2f4d1","7b72f18b914348eb84c74e6260ddb927","66e9fece8d29418c93fd9fa46a04e29d","2be40fe19de14eaf8e204679d21737da"]},"id":"-UynOo1tTImZ","executionInfo":{"status":"ok","timestamp":1757565909813,"user_tz":-330,"elapsed":189369,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}},"outputId":"e5b3d4ec-43bb-4155-8e0b-a7922c1bb214"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c45d1b9de84423391bf33f3f94df6dc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50/50 02:21, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Generated 1: If i would be the Prime Minister of India..\n","\n","I will not be the Minister of the United Kingdom. I will not be a member of any government that is of the United Nations.\n","\n","I will not be a member of any government that is of the United Kingdom.\n","\n","I will not be a member of any government that is of the United States.\n","\n","I will not be a member of any government that is of the United States.\n","\n","I will not be a member any government that is of the United States.\n","\n","I will not be a member any government that is of the United States.\n","\n","I will not be a member to the United States.\n","\n","I will not be a to the United States. I will not be a member of any government that is of the United States.\n","\n","I will not be a to the United States.\n","\n","I will not be a to the United States.\n","\n","I will not be a member of any government that is of the United States.\n","\n","I will not be a to the United States.\n","\n","I will not be a to the United States.\n","I will not be a to the United States.\n","\n","I will not be a to the United States.\n","\n","I will not be a to the\n","\n","Generated 2: If i would be the Prime Minister of India..\n","\n","that I am the Prime Minister of India and India\n","\n","(and India)\n","\n","and I am the Prime Minister of India and India\n","\n","\"\n","\n","(That is the reason I said that one of the three\n","\n","the three reasons why I am the Prime Minister of India\n","\n","and India\n","\n","that I\n","\n","why I said that one of the reasons why is why I\n","\n","why I have said that one of the reasons why he, one of the reasons why is\n","\n","why I\n","\n","that one of the reasons why I\n","\n","(that one of the reasons why he, one of the reasons why I have\n","\n","that one of the reasons why I\n","\n","(that one of the reasons why that one of the reasons why I that one of the reasons why\n","\n","that one of the reasons why I\n","\n","that one of the reasons why I\n","that one of the reasons why I\n","\n","that one of the reasons why I\n","\n","that I\n","\n","that one of the reasons why I\n","\n","that one of the reasons why I\n","\n","that one of the reasons why I\n","\n","that one of the reasons why I\n","\n","that one of the reasons why I\n","\n","that one of the reasons why\n"]}],"source":["# !pip install transformers datasets peft accelerate bitsandbytes\n","\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n","from peft import LoraConfig, get_peft_model\n","import os\n","\n","os.environ[\"WANDB_DISABLED\"] = \"true\"   # disable wandb\n","\n","# 1. Load model + tokenizer\n","model_name = \"gpt2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# 2. Apply LoRA config\n","peft_config = LoraConfig(task_type=\"CAUSAL_LM\", r=8, lora_alpha=16, lora_dropout=0.1)\n","model = get_peft_model(model, peft_config)\n","\n","# 3. Tiny dataset with labels\n","def preprocess(batch):\n","    tokens = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n","    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n","    return tokens\n","\n","dataset = load_dataset(\"yelp_polarity\", split=\"train[:200]\").map(preprocess, batched=True)\n","\n","# 4. Training\n","args = TrainingArguments(\n","    output_dir=\"out\",\n","    per_device_train_batch_size=4,\n","    num_train_epochs=1,\n","    report_to=\"none\"\n",")\n","trainer = Trainer(model=model, args=args, train_dataset=dataset)\n","trainer.train()\n","\n","from transformers import pipeline\n","\n","# Load the fine-tuned model + tokenizer from Trainer\n","text_generator = pipeline(\n","    \"text-generation\",\n","    model=trainer.model,\n","    tokenizer=tokenizer,\n","    device=0  # set -1 if running on CPU\n",")\n","\n","# Try with a custom prompt\n","prompt = \"If i would be the Prime Minister of India..\"\n","outputs = text_generator(prompt, max_length=50, num_return_sequences=2, do_sample=True, top_k=50)\n","\n","# Print results\n","for i, out in enumerate(outputs):\n","    print(f\"\\nGenerated {i+1}: {out['generated_text']}\")\n","\n"]}]}