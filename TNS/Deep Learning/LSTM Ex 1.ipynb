{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOqr4UtIo2dsN3oGs9Yyzb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOqokCQw8PIw","executionInfo":{"status":"ok","timestamp":1756825660145,"user_tz":-330,"elapsed":451316,"user":{"displayName":"Nayna Sagar Dahatonde","userId":"08351810259517213934"}},"outputId":"398e638d-c818-469b-dd3b-24907c6fc381"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Training samples: 25000\n","Testing samples: 25000\n","Epoch 1/2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 465ms/step - accuracy: 0.6890 - loss: 0.5642 - val_accuracy: 0.8472 - val_loss: 0.3602\n","Epoch 2/2\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 485ms/step - accuracy: 0.8867 - loss: 0.2850 - val_accuracy: 0.8590 - val_loss: 0.3319\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 99ms/step - accuracy: 0.8512 - loss: 0.3429\n","Test Accuracy: 0.8531200289726257\n"]}],"source":["# Import libraries\n","import tensorflow as tf\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# 1. Load dataset (IMDB movie reviews)\n","# num_words=10000 -> only keep top 10,000 most common words\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n","\n","print(\"Training samples:\", len(x_train))\n","print(\"Testing samples:\", len(x_test))\n","\n","# 2. Preprocess data\n","# Reviews have different lengths → make them equal (padding)\n","max_len = 200  # keep only first 200 words\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen=max_len)\n","\n","# 3. Build LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_len))  # word embeddings\n","model.add(LSTM(128))   # LSTM layer with 128 memory units\n","model.add(Dense(1, activation='sigmoid'))  # output layer (0 = negative, 1 = positive)\n","\n","# 4. Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# 5. Train the model\n","history = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_split=0.2)\n","\n","# 6. Evaluate the model\n","loss, accuracy = model.evaluate(x_test, y_test)\n","print(\"Test Accuracy:\", accuracy)\n"]}]}